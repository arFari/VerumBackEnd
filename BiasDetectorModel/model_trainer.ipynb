{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if not already installed)\n",
    "#pip install transformers datasets torch\n",
    "\n",
    "# Mount Google Drive to access files stored there\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, set_seed)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(239)\n",
    "\n",
    "# Check device (Colab should have GPU enabled if available)\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 1. Data Extraction from CSV file on Google Drive\n",
    "csv_file_path = \"/content/drive/MyDrive/news_bias.csv\"  # Update with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(\"Data preview:\")\n",
    "print(df.head())\n",
    "print(\"Original label distribution:\")\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "# 1.1. Sanitize the dataset (example: strip and lowercase)\n",
    "def sanitize_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "df['text'] = df['text'].apply(sanitize_text)\n",
    "\n",
    "# 1.2. Merge labels: combine label 0 and 2 into 0, keep label 1 unchanged.\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x in [0, 2] else 1)\n",
    "\n",
    "print(\"Modified label distribution after merging:\")\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "# 2. Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 3. Split dataset: 70% train, 15% validation, and 15% test using seed 239.\n",
    "split_dataset = dataset.train_test_split(test_size=0.3, seed=239)\n",
    "val_test = split_dataset['test'].train_test_split(test_size=0.5, seed=239)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': val_test['train'],\n",
    "    'test': val_test['test']\n",
    "})\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(\"Train size:\", len(dataset_dict['train']))\n",
    "print(\"Validation size:\", len(dataset_dict['validation']))\n",
    "print(\"Test size:\", len(dataset_dict['test']))\n",
    "\n",
    "# 4. Balance the training set to achieve a 50:50 ratio.\n",
    "# Convert the training set to a pandas DataFrame for undersampling.\n",
    "train_df = dataset_dict['train'].to_pandas()\n",
    "\n",
    "# Separate the classes.\n",
    "df_class0 = train_df[train_df['label'] == 0]\n",
    "df_class1 = train_df[train_df['label'] == 1]\n",
    "\n",
    "# Determine the minimum count among classes.\n",
    "min_count = min(len(df_class0), len(df_class1))\n",
    "\n",
    "# Undersample each class to the minimum count.\n",
    "df_class0_under = df_class0.sample(n=min_count, random_state=239)\n",
    "df_class1_under = df_class1.sample(n=min_count, random_state=239)\n",
    "\n",
    "# Combine and shuffle to get the balanced training dataframe.\n",
    "balanced_train_df = pd.concat([df_class0_under, df_class1_under]).sample(frac=1, random_state=239).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced training label distribution:\")\n",
    "print(balanced_train_df['label'].value_counts(normalize=True))\n",
    "\n",
    "# Convert the balanced training DataFrame back to a Hugging Face Dataset.\n",
    "balanced_train_dataset = Dataset.from_pandas(balanced_train_df)\n",
    "\n",
    "# Replace the training set in dataset_dict with the balanced training set.\n",
    "dataset_dict['train'] = balanced_train_dataset\n",
    "\n",
    "print(\"Final split sizes after balancing training set:\")\n",
    "print(\"Train size:\", len(dataset_dict['train']))\n",
    "print(\"Validation size:\", len(dataset_dict['validation']))\n",
    "print(\"Test size:\", len(dataset_dict['test']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Pass the compute_metrics function\n",
    ")\n",
    "\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test evaluation results:\")\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model and tokenizer as shown before:\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "checkpoint_path = \"results/checkpoint-755\"\n",
    "export_path = \"exported_model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "model.save_pretrained(export_path)\n",
    "tokenizer.save_pretrained(export_path)\n",
    "\n",
    "print(\"Model and tokenizer have been exported to\", export_path)\n",
    "\n",
    "# Compress the exported model directory into a ZIP file\n",
    "!zip -r exported_model.zip exported_model\n",
    "\n",
    "# For Google Colab, use the files module to download the ZIP file\n",
    "from google.colab import files\n",
    "files.download(\"exported_model.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
